{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FZIZmHunYRF"
      },
      "source": [
        "### 1. 토큰화 (Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkJPT7uGoHD9",
        "outputId": "682c2b77-520a-461d-ac60-ae59be4fe065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xh27etooU5m"
      },
      "source": [
        "### 1) 단어 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pN0KgLYZocXh"
      },
      "outputs": [],
      "source": [
        "sample  = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMjgXczTpjST",
        "outputId": "dcf111ed-eb9c-403d-e5f9-8e9fc2513fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sample))     # sample 토큰화 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk0kHkjnpprq",
        "outputId": "cd26b481-6e75-42e3-b713-a8fa453b8671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wpt = WordPunctTokenizer()\n",
        "print(wpt.tokenize(sample))    # 구두점을 분리하는 것이 특징"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL2L8qtNqRvO",
        "outputId": "04a16e22-8787-48c4-ed24-16396e386917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJRYITJsq58C",
        "outputId": "1d68f2fa-eb17-4de4-89a9-0fcd6bec376d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tok = TreebankWordTokenizer()\n",
        "print(tok.tokenize(sample))   # 표준화르 쓰이는 것 중 하나"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpbIS9I0recd"
      },
      "source": [
        "### 2) 문장 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwGihcbArmBz",
        "outputId": "59ac2e5b-65bd-4f55-a542-d8b3dc2b32ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ],
      "source": [
        "# 문장단위로 나온 텍스트 토큰화\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI_rFQ2HsJxu",
        "outputId": "6629cbd5-1918-43a6-dcd2-38f222af8e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['His', 'barber', 'kept', 'his', 'word', '.']\n",
            "['But', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'him', 'crazy', '.']\n",
            "['Finally', ',', 'the', 'barber', 'went', 'up', 'a', 'mountain', 'and', 'almost', 'to', 'the', 'edge', 'of', 'a', 'cliff', '.']\n",
            "['He', 'dug', 'a', 'hole', 'in', 'the', 'midst', 'of', 'some', 'reeds', '.']\n",
            "['He', 'looked', 'about', ',', 'to', 'make', 'sure', 'no', 'one', 'was', 'near', '.']\n"
          ]
        }
      ],
      "source": [
        "# sentence -> word 로\n",
        "# b언어의 감성...?)()\n",
        "for sentence in sent_tokenize(text):\n",
        "    print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVk1Tv70si8s",
        "outputId": "72f08e2c-d632-4ab6-d82f-d9d9c79fa5aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]\n"
          ]
        }
      ],
      "source": [
        "#Ph.D. 이거 따로 분리 안하고 잘 가져옴 \n",
        "\n",
        "text =\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ip0SSJktCFu"
      },
      "source": [
        "- 한글 문장 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vYQ_KX3itM63"
      },
      "outputs": [],
      "source": [
        "# KSS (Korean Sentence Splitter) 설치\n",
        "\n",
        "# !pip install kss\n",
        "\n",
        "# 파일 다운받은거 위치 확인 가능!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kpE-JckStUDu"
      },
      "outputs": [],
      "source": [
        "# KSS (Korean Sentence Splitter) 설치시 메세지를 보고 싶지 않을 때\n",
        "!pip install kss > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg-SRzfSuDF9",
        "outputId": "ecd0b8b3-4a0a-4f52-c841-3135b06ce1b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Korean Sentence Splitter]: Initializing Pynori...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ],
      "source": [
        "import kss \n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc3Gvwu41JZr"
      },
      "source": [
        "### 3) 품사 (POS : Part-of-Speech) 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZpgbKxr1atf",
        "outputId": "2586e10b-a6e7-4808-9928-792a6c1d20d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bElW-MRY1iSW",
        "outputId": "7c1147db-f3e4-4c80-a032-92db58cdce04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "078rBfh01y86",
        "outputId": "d24090f4-b4fe-46a0-a4cf-4fe1dbaea80c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1MtjdLO18nu"
      },
      "source": [
        "- 한글 (KoNLPy:코엔엘파이) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLluxD5B3hZP",
        "outputId": "64ddf105-44bd-4373-ec96-797cb1aa72db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->Konlpy) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "#KoNLPy 설치\n",
        "!pip install Konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqAmx1nx3pmR"
      },
      "source": [
        "##### okt(Open Korean Text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II2qmurV4S_g",
        "outputId": "500b80cb-c7d9-40c5-fc1b-125eebb44b3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 형태소 분석\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "okt.morphs(text)   # 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIIiGPzn4YEu",
        "outputId": "edd35905-bd41-4db0-9fdb-b3cc9932565c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가보다']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "okt.morphs(text, stem=True)   # stem=True 동사 원형 추출 / 가봐요 -> 가보다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDjx2LmH4v8e",
        "outputId": "45487587-9c05-4074-e5ed-eccb564aa2ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('열심히', 'Adverb'),\n",
              " ('코딩', 'Noun'),\n",
              " ('한', 'Josa'),\n",
              " ('당신', 'Noun'),\n",
              " (',', 'Punctuation'),\n",
              " ('연휴', 'Noun'),\n",
              " ('에는', 'Josa'),\n",
              " ('여행', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('가봐요', 'Verb')]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 품사 부착\n",
        "okt.pos(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGPopPWx5F0I",
        "outputId": "3e81502b-3126-4d1d-abe3-2f6a2175f326"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 명사 추출\n",
        "okt.nouns(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pDocLWF5VE3"
      },
      "source": [
        "#####꼬꼬마"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4L_kgCN57wP",
        "outputId": "246e1518-71b5-497f-97f8-f19fd693a9aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma.morphs(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdKzYX5p6JYa"
      },
      "source": [
        "뭐 할지 고민하지 말고 그냥 아무거나 잡아서 쓰면 된다(?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akegORC56qjz"
      },
      "source": [
        "### 02) 정제(Cleaning) and 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fzH7tIfy_eoz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pmXvX8Lm_k0t",
        "outputId": "6786d4d1-7e99-4170-81e5-10fedcf1249c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' was wondering anyone out there could enlighten this car.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "# 강사님은 이 방법 비추....ㅋㅋ\n",
        "# 작은애들은 이걸루 \n",
        "\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "shortword.sub('',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TDO-3By_yOt",
        "outputId": "bfa4b47c-e4cf-48bd-8cd4-3ab575510ade"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['was',\n",
              " 'wondering',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 단어 토큰화한 후 기리가 2보다 큰 단어 발췌\n",
        "\n",
        "[word for word in word_tokenize(text) if len(word) > 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oTbqSACjANRj",
        "outputId": "0cec8d1c-56bc-476a-fb92-f5f998133f49"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was wondering anyone out there could enlighten this car'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text = ' '.join([word for word in word_tokenize(text) if len(word) > 2])\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xATVwTBAo6O"
      },
      "source": [
        "### 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DFI7xUhBR3t"
      },
      "source": [
        "##### 1) 표제어 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZwO_JojCbMk",
        "outputId": "bac8858e-6568-457b-92c6-f86616c88c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "r7U491HZCeKk"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keDYfnlICnxT",
        "outputId": "563dae82-6793-49f6-f321-65d3c3f76d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ],
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lemma.lemmatize(word) for word in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noXvfA2hCxLb",
        "outputId": "35c4933f-94e5-40b2-eb39-849f0f0c2e45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('do', 'die', 'watch', 'have')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 품사 붙이니 원형 제대로 추출됨\n",
        "\n",
        "lemma.lemmatize('doing', 'v'),lemma.lemmatize('dies', 'v'),lemma.lemmatize('watched', 'v'),lemma.lemmatize('has', 'v'),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCz6rYLDdzp",
        "outputId": "d3a61536-5698-4fed-cdbe-0c134fda9b67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('live', 'life')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemma.lemmatize('lives', 'v'), lemma.lemmatize('lives', 'n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdx_yub_DvEC"
      },
      "source": [
        "#### 2) 어간 추출(Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xFrF8idUEiBn"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer()\n",
        "\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1HeGrMfE3VE",
        "outputId": "60431914-6c12-458a-b972-b7ab94859081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ],
      "source": [
        "# 어간 추출 전\n",
        "print(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME9G6z8sE7DM",
        "outputId": "c13e3a8a-c284-4baf-82fe-4608372c337d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ],
      "source": [
        "# 어간 추출 후\n",
        "print([ps.stem(word) for word in word_tokenize(text)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbpZoZ2-FCMo",
        "outputId": "e6e4bf36-a80e-4977-c666-9ab1e4fa444c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ],
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([ps.stem(word) for word in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz-srP1vFdQg",
        "outputId": "7f64adc4-caba-49d6-f3d0-cd994b209191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ],
      "source": [
        "# Porter Stemmer\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([ps.stem(word) for word in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiJpKA7cF6tG"
      },
      "outputs": [],
      "source": [
        "# Lancaster Steemer\n",
        "from nltk.stem import LancasterStemmer\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "11.텍스트 전처리",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
